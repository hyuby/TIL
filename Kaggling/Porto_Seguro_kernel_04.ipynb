{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.feature_selection import VarianceThreshold \n#feature의 variance가 적다는 말은 정보가 별로 없다는 말일 수도 있다. 100개의 샘플의 데이터가 99개 1 1개 0이면\n#별로 배울게 없는 데이터이다.\nfrom sklearn.feature_selection import SelectFromModel\n#모델의 중요도에 따라서 선택하는 것.\n#어떤 특성은 0으로 예측 했을 때 중요한 특성이 1로 하니까 중요도가 안 좋아지는 경우 ==> 중요한 특성\n#그런데 0으로 하든 1로 하든 중요도가 변함이 없는 경우 ==> 안중요\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_rows', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG :\n    NROWS = 500000\nelse :\n    NROWS = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 읽을 데이터가 너무 크다!\n* pd.read_csv('....', nrows=integer) 또는\n* sample의 fraction을 이용하는 것도 한 방법(random성 있음)\n* imbalance data가 걱정 되는 경우 StratifiedSampling 이용\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntrain = train.sample(frac=0.1)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stratified Sampling 깔끔하게 하기 꿀팁!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fold = StratifiedKFold(n_splits=10, random_state =42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# index뽑기\nfor trn_idx, tst_idx in fold.split(train, train['target']):\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trn_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.iloc[trn_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.iloc[tst_idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"깔끔!"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [col for col in train.columns if 'cat' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[cat_cols[0]].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# category의 유니크한 개수\nfor col in cat_cols:\n    print(col,':', train[col].nunique() )\n    #high cardinality : ps_cal_11_cat ==> 104개 \n    #(one-hot-encoding하게 되면 머리아파짐,, 다른 encoding방법이 필요)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop_duplicates()\ntrain.shape\n#중복된 값 없음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop_duplicates()\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"dummy feature에 one_hot_encoding을 실시할 것"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info() #object는 없다 --> label_encoding 필요 없음","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metadata\nTo facilitate the data management, we'll store meta-inforamtation about the variablel in a DataFrame. This will be helpful when we wnat to select specific variables for analysis, visualization, modeling,...\n\nConcretely we will store:\n* role : input, ID, target\n* level : nominal, interval, ordinal, binary\n* keep : True, False\n* dtype : int, float, str\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\nfor f in train.columns:\n    if f == 'target' :\n         role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else : \n        role = 'input'\n        \n    #Defining the level\n    if 'bin' in f or f == 'target':\n        level =  'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    #Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    #Defining the data type    \n    dtype = train[f].dtype\n    #Crearting a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname':f,\n        'role':role,\n        'level':level,\n        'keep': keep,\n        'dtype':dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns = ['varname','role','level','keep','dtype'])\nmeta.set_index('varname',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Example to extract all nominal variables that are not dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below the number of variables per role and level are displayed"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'count': meta.groupby(['role','level'])\n             ['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"정리를 이쁘게 했다."},{"metadata":{},"cell_type":"markdown","source":"### Descriptive Statistics\nWe can also apply the describe method on the dataframe. However, it doesn't make much sense to calculate the mean, std,... on categorical variables and the id variable. We'll explore the categorical variables visually later.\n\nThanks to our meta file we can easily select the variables on which we want to compute the descriptive statistics. To keep things clear, we'll do this per data type."},{"metadata":{},"cell_type":"markdown","source":"Interval variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feat = [col for col in train.columns if 'cat' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feat #이런식으로 뽑는 것도 가능하다","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"reg variables\n* only ps_reg_03 has missing values\n* the range(min to max) differs between the variables. We could apply scaling(e.g StandardScaler), but it depends on the classifier we will want to use.\n* linear regression일 경우, feature들 간의 distribution이 너무 다르, 특정 var에 쏠리는 경우가 발생함. StandardScaler사용.(선형모델)\n* Tree 모델을 사용할 경우, 절대값의 크기에 영향 별로 안받음. 굳이 안해도 됨"},{"metadata":{},"cell_type":"markdown","source":"car variables\n* ps_car_12 and ps_car_15 have missing values\n* again, the range differs and we could apply scaling."},{"metadata":{},"cell_type":"markdown","source":"calc variables\n* no missing values\n* this seems to be some kind of ratio as the maximum is 0.9\n* all three `_calc` variables have very similar distributions"},{"metadata":{},"cell_type":"markdown","source":"__Overall,__ we can see that the range of the interval variables is rather small. Perhaps some transformation(e.g log) is already applied in order to anoymize the data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ordinal variables\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Only one missing variable: ps_car_11\n* We could apply sacling to deal with the different ranges"},{"metadata":{},"cell_type":"markdown","source":"Binary variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- A priori in the train data is 3.645%, which is __strongly imbalanced.__\n- From the means we can conclude that for most variables the value is zero in most cases. (대부분이 보험 청구가 안되었다.)"},{"metadata":{},"cell_type":"markdown","source":"### Handling imbalanced classes\nAs we mentioned above the proportion of records with target = 1 is far less than target=0. This lead to a model that has great accuracy but does have any added value in practice. Two possible strategies to deal with this problem are:\n- oversampling records with target=1\n- undersampling records with target=0\nThere are many more strategies of course and MachineLearningMastery.com gives a nice overview. As we have a rather large training set, se can go for __undersamplling.__"},{"metadata":{},"cell_type":"markdown","source":"얼마나 1을 잘 맞추느냐가 중요한 상황. gini나 ROC AUC를 사용하는 이유.  \nundersampling 많은 걸 줄임, target이 0인 걸 줄일 것\n"},{"metadata":{},"cell_type":"markdown","source":"SMOTE!"},{"metadata":{"trusted":true},"cell_type":"code","source":"desired_apriori = 0.10\n\n#Get the indices per target value\nidx_0 = train[train.target ==0].index\nidx_1 = train[train.target == 1].index\n\n#Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target =0\nundersampling_rate = ((1-desired_apriori)*nb_1) / (nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n#Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples = undersampled_nb_0)\n\n#construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n#return undersampled data frame\ntrain = train.loc[idx_list].reset_index(drop=True) #shuffle해줬으니까 순서대로","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Quality Checks"},{"metadata":{},"cell_type":"markdown","source":"* Checking missing values  \nMissings are represeted as -1"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars_with_missing = []\nfor f in train.columns :\n    missings = train[train[f] == -1][f].count()\n    if missings > 0 :\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ps_car_03_cat and ps_car_05_cat have a large proportion of recoreds with missing values. Remove these variables.\n- For the other categorical variables with missing values, we can leave the missing value -1 as such.\n- ps_reg_03(continuois) has missing values for 18% of all records. Replace by the mean\n- ps_car_11(ordinal) has only 5 records with missing values. Replace by the mode.\n- ps_car_12(continuous)has only 1records with missing value. Replace by the mean.\n- ps_car_14(continuous) has missing values for 7% of all records. Replace by the mean."},{"metadata":{},"cell_type":"markdown","source":"### Missing Value는 함부로 채우면 안된다\n모델을 학습시켜서 채우는 방법도 있다."},{"metadata":{},"cell_type":"markdown","source":"가끔씩 missing value값이 확 뛸 때가 있음. 이경우는 missing value에 정보가 있다는 말일 수 도 있음. 하지만 porto에서는 정보가 없다."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop), 'keep'] = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_reg_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mean_imp.fit_transform(train[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_imp.fit_transform(train[['ps_car_14']]).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checing the cardinality of the categorical variables\nCardinality refers to the number of different values in a variable. As we will create dummy variables from the categorical variables later on, we need to check whether there are variables with many distinct values. We should handle these variables differently as they would result in many dummy variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level =='nominal') & (meta.keep)].index\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Varaible {} has {} distinct values'.format(f, dist_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[v[0]].value_counts().shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[v[0]].nunique() # 같은 결과","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One hot Encoding 하는데 Cardinality가 높다면, 많은 distinct values들이 있다면, 1000개의 칼럼 계산할게 너무 많아짐."},{"metadata":{},"cell_type":"markdown","source":"Only ps_car_11 has many distinct values, although it is still reasonable.\n\nEDIT: nickycan made an excellent remark on the fact that my first solution could lead to data leakage. He also pointed me to another kernel made by oliver which deals with that. I therefore replaced this part with the kernel of oliver. All credits go to him. It is so great what you can learn by participating in the Kaggle competitions :)"},{"metadata":{},"cell_type":"markdown","source":"74개의 칼럼이 생기게 되는데...Accept할지 말지는 알아서 결정."},{"metadata":{},"cell_type":"markdown","source":"칼럼이 너무 많아지면, 트리모델을 사용할 경우, 트리의 깊이도 깊어지고, 중요한 질문을 못 건드릴 가능성 높아짐, 답으로 가기 위한 과정에서 멀어짐, 원하지 않는 split이 생길 수 있음. one hot encoding많아지면 힘들다... ==> mean encoding(Overfitting 가능성, 그래서 noise를 추가해줌) OR Frequency Encoding 사용한다."},{"metadata":{},"cell_type":"markdown","source":"## Mean Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_series = train[\"ps_car_11_cat\"]\ntst_series = test[\"ps_car_11_cat\"]\ntarget = train.target\nmin_samples_leaf = 100\nsmoothing = 10\nnoise_level = 0.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.concat([trn_series, target], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.groupby(by = trn_series.name)[target.name].agg(['mean', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution smoothing\nsmoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoothing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior = target.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_trn_series = pd.merge(trn_series.to_frame(trn_series.name), \n                        averages.reset_index().rename(columns = \n                            {'index':target.name, target.name : 'average'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averages #target의 mean을 smoothing한 정보만 남는다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_series.to_frame(trn_series.name), averages.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averages.reset_index().rename(columns={'index' : target.name, target.name : 'average'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns = {'index':target.name,\n                                                target.name:'average'})\n,on = tst_series.name, how = 'left')['average'].rename(trn_series.name + '_mean').fillna(prior)    \n#left는 꼭 해줄 것, 안해주면 샘플이 빠지는 경우가 발생","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_trn_series.index = trn_series.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#노이즈 추가, 너무 discrete하지 않게 continuous하게 만들어준다.\nadd_noise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"0.01 * np.random.randn(ft_trn_series.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ps_car_11_cat_te'] = train_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"#### Categorical variables\nLets' look into the categorical varaibles and the proportion of customers with target=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'nominal')& (meta.keep)].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=2)\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"As we can see from the variables with missing values, it is a good idea to keep the missing values as a separate category value, instead of replacing them by the mode for instance. The customers with a missing value appear to have a much higher (in some cases much lower) probability to ask for an insurance claim."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Interval variables\nChecking the correlations between interval variables. A heatmap is a good way to visualize the correlation between variables. The code below is based on an example by Michael Waskom"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"There are a strong correlations between the variables:\n\n- ps_reg_02 and ps_reg_03 (0.7)\n- ps_car_12 and ps_car13 (0.67)\n- ps_car_12 and ps_car14 (0.58)\n- ps_car_13 and ps_car15 (0.67)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Seaborn has some handy plots to visualize the (linear) relationship between variables. We could use a pairplot to visualize the relationship between the variables. But because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.\nNOTE: I take a sample of the train data to speed up the process.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"s = train.sample(frac=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"ps_reg_02 and ps_reg_03\nAs the regression line shows, there is a linear relationship between these variables. Thanks to the hue parameter we can see that the regression lines for target=0 and target=1 are the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ps_car_12 and ps_car_13"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"ps_car_12 and ps_car_14"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"ps_car_13 and ps_car_15¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Allright, so now what? How can we decide which of the correlated variables to keep? We could perform Principal Component Analysis (PCA) on the variables to reduce the dimensions. In the AllState Claims Severity Competition I made this kernel to do that. But as the number of correlated variables is rather low, we will let the model do the heavy-lifting."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Checking the correlations between ordinal variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For the ordinal variables we do not see many correlations. We could, on the other hand, look at how the distributions are when grouping by the target value."},{"metadata":{},"cell_type":"markdown","source":"단순히 숫자만 뽑을 것이 아니라, 숫자를 믿지 말고 일일이 다시 그림을 그려보면서 분포에 맞는 값이 나온 건지 재확인 해봐야한다.(진짜 데이터포인트가 선형으로 되어있는지 아님 선형적인게 없는데 높은건지 확인을해야된다. 수치에 속지 말자)"},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"Creating dummy variables\nThe values of the categorical variables do not represent any order or magnitude. For instance, category 2 is not twice the value of category 1. Therefore we can create dummy variables to deal with that. We drop the first dummy variable as this information can be derived from the other dummy variables generated for the categories of the original variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, creating dummy variables adds 52 variables to the training set."},{"metadata":{},"cell_type":"markdown","source":"### Creating interaction variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level =='interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n# interaction_only=True면 자신 제곱하는 것 제외됨, \ninteractions = pd.DataFrame(data = poly.fit_transform(train[v]), columns = poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly.fit_transform(train[v])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly.get_feature_names(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions #많은 feature들이 생긴 것을 알 수 있다.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This adds extra interaction variables to the train data. Thanks to the get_feature_names method we can assign column names to these new variables."},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"* Removing features with low or zero variance  \nPersonally, I prefer to let the classifier algorithm chose which features to keep. But there is one thing that we can do ourselves. That is removing features with no or a very low variance. Sklearn has a handy method to do that: VarianceThreshold. By default it removes features with zero variance. This will not be applicable for this competition as we saw there are no zero-variance variables in the previous steps. But if we would remove features with less than 1% variance, we would remove 31 variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id','target'],axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = np.vectorize(lambda x : not x)\nv = train.drop(['id','target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selector.get_support()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f(selector.get_support()) #위 값을 뒤집어 준다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"~selector.get_support() #이것도 가능","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We would lose rather many variables if we would select based on variance. But because we do not have so many variables, we'll let the classifier chose. For data sets with many more variables this could reduce the processing time.\n\nSklearn also comes with other feature selection methods. One of these methods is SelectFromModel in which you let another classifier select the best features and continue with these. Below I'll show you how to do that with a Random Forest"},{"metadata":{},"cell_type":"markdown","source":"### Selecting features with a Random Forest and SelectFromModel\nHere we'll base feature selection on the feature importances of a random forest. With Sklearn's SelectFromModel you can then specify how many variables you want to keep. You can set a threshold on the level of feature importance manually. But we'll simply select the top 50% best variables.\n\nThe code in the cell below is borrowed from the GitHub repo of Sebastian Raschka. This repo contains code samples of his book Python Machine Learning, which is an absolute must to read."},{"metadata":{},"cell_type":"markdown","source":"### 1000개의 피쳐를 일일이 성능보고 셀렉션 하면 너무 오래걸림\n- 블록을 사용하자\n- 10개씩 20개씩 묶어서 사용\n"},{"metadata":{},"cell_type":"markdown","source":"- 20개의 모델 베이스라인을 만들고\n- +random choosing 20\n- 40개의 모델 학 -> 베이스모델과 비교\n- if 성능 향상 -> feature importance 상위 10%에 + 새로 추가된 거 생기면\n- 걔를 남기고 \n- 안생기면 random choosing, 반복"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id','target'], axis=1)\ny_train = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_labels = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=20, random_state=0, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train[:2000],y_train[:2000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.feature_importances_ #갯수가 너무 적어서 안나온다","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.argsort(rf.feature_importances_)[::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"with SelectFromModel we can specify which prefit classifier to use and what the threshold is for the feature importances. With the get_support method we can then limit the number of variables in the train data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[selected_vars + ['target']] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling\nAs mentioned before, we can apply standard scaling to the training data. Some classifiers perform better when this is done."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'],axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 주의!\nscaler.fit_trainform(train)  \nscaler.transform(test) 이렇게 해주어야 함.  \n훈련셋으로 훈련시키고 테스트셋에 적용시키기"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}