{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"titanic\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/titanic/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = spark.read.load(path,\n",
    "                   format='csv',\n",
    "                   header='true',\n",
    "                   inferSchema='true').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n"
     ]
    }
   ],
   "source": [
    "print(titanic_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+\n",
      "|Survived|Pclass|Embarked|\n",
      "+--------+------+--------+\n",
      "|       0|     3|       S|\n",
      "|       1|     1|       C|\n",
      "|       1|     3|       S|\n",
      "|       1|     1|       S|\n",
      "|       0|     3|       S|\n",
      "|       0|     3|       Q|\n",
      "|       0|     1|       S|\n",
      "|       0|     3|       S|\n",
      "|       1|     3|       S|\n",
      "|       1|     2|       C|\n",
      "|       1|     3|       S|\n",
      "|       1|     1|       S|\n",
      "|       0|     3|       S|\n",
      "|       0|     3|       S|\n",
      "|       0|     3|       S|\n",
      "|       1|     2|       S|\n",
      "|       0|     3|       Q|\n",
      "|       1|     2|       S|\n",
      "|       0|     3|       S|\n",
      "|       1|     3|       C|\n",
      "+--------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.select(\"Survived\",\"Pclass\",\"Embarked\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|   Sex|Survived|count|\n",
      "+------+--------+-----+\n",
      "|  male|       0|  468|\n",
      "|female|       1|  233|\n",
      "|female|       0|   81|\n",
      "|  male|       1|  109|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.groupBy(\"Sex\",\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|Pclass|Survived|count|\n",
      "+------+--------+-----+\n",
      "|     1|       0|   80|\n",
      "|     3|       1|  119|\n",
      "|     1|       1|  136|\n",
      "|     2|       1|   87|\n",
      "|     2|       0|   97|\n",
      "|     3|       0|  372|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.groupBy(\"Pclass\",\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_count(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns_count_list = null_value_count(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+\n",
      "|Column_With_Null_Value|Null_values_count|\n",
      "+----------------------+-----------------+\n",
      "|                   Age|              177|\n",
      "|                 Cabin|              687|\n",
      "|              Embarked|                2|\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_values_count']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = titanic_df.select(mean('Age')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| Initial|\n",
      "+--------+\n",
      "|     Don|\n",
      "|    Miss|\n",
      "|Countess|\n",
      "|     Col|\n",
      "|     Rev|\n",
      "|    Lady|\n",
      "|  Master|\n",
      "|     Mme|\n",
      "|    Capt|\n",
      "|      Mr|\n",
      "|      Dr|\n",
      "|     Mrs|\n",
      "|     Sir|\n",
      "|Jonkheer|\n",
      "|    Mlle|\n",
      "|   Major|\n",
      "|      Ms|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.select('Initial').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.replace(['Mlle','Mme', 'Ms', 'Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "               ['Miss','Miss','Miss','Mr','Mr',  'Mrs',  'Mrs',  'Other',  'Other','Other','Mr','Mr','Mr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|   Miss|\n",
      "|  Other|\n",
      "| Master|\n",
      "|     Mr|\n",
      "|    Mrs|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.select('Initial').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Miss\") & (titanic_df[\"Age\"].isNull()), 22).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Other\") & (titanic_df[\"Age\"].isNull()), 46).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Master\") & (titanic_df[\"Age\"].isNull()), 5).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Mr\") & (titanic_df[\"Age\"].isNull()), 33).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Mrs\") & (titanic_df[\"Age\"].isNull()), 36).otherwise(titanic_df[\"Age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|     Mr|\n",
      "|     Mr|\n",
      "|     Mr|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.filter(titanic_df.Age==46).select('Initial').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| Age|\n",
      "+----+\n",
      "|22.0|\n",
      "|38.0|\n",
      "|26.0|\n",
      "|35.0|\n",
      "|35.0|\n",
      "|33.0|\n",
      "|54.0|\n",
      "| 2.0|\n",
      "|27.0|\n",
      "|14.0|\n",
      "| 4.0|\n",
      "|58.0|\n",
      "|20.0|\n",
      "|39.0|\n",
      "|14.0|\n",
      "|55.0|\n",
      "| 2.0|\n",
      "|33.0|\n",
      "|31.0|\n",
      "|36.0|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.select('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   77|\n",
      "|    null|    2|\n",
      "|       C|  168|\n",
      "|       S|  644|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.groupBy('Embarked').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.na.fill({'Embarked' : 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = false)\n",
      " |-- Initial: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn('Family_Size', col('SibSp')+col('Parch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn('Alone', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Alone\",when(titanic_df[\"Family_Size\"] == 0, 1).otherwise(titanic_df[\"Alone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = titanic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Output column Sex_index already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.0/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o448.fit.\n: java.lang.IllegalArgumentException: requirement failed: Output column Sex_index already exists.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:91)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:109)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c53567512f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Sex\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Embarked\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Initial\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtitanic_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-c53567512f58>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Sex\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Embarked\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Initial\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtitanic_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.0/libexec/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.0/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Output column Sex_index already exists.'"
     ]
    }
   ],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(titanic_df) for column in [\"Sex\",\"Embarked\",\"Initial\"]]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "titanic_df = pipeline.fit(titanic_df).transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_16d7ed66556f"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Initial|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+---------+--------------+-------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|     Mr|          1|    0|      0.0|           0.0|          0.0|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|    Mrs|          1|    0|      1.0|           1.0|          2.0|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|   Miss|          0|    1|      1.0|           0.0|          1.0|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|    Mrs|          1|    0|      1.0|           0.0|          2.0|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|33.0|    0|    0|          330877| 8.4583|       Q|     Mr|          0|    1|      0.0|           2.0|          0.0|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S| Master|          4|    0|      0.0|           0.0|          3.0|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|    Mrs|          2|    0|      1.0|           0.0|          2.0|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|    Mrs|          1|    0|      1.0|           1.0|          2.0|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|   Miss|          2|    0|      1.0|           0.0|          1.0|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|   Miss|          0|    1|      1.0|           0.0|          1.0|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|     Mr|          6|    0|      0.0|           0.0|          0.0|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|   Miss|          0|    1|      1.0|           0.0|          1.0|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|    Mrs|          0|    1|      1.0|           0.0|          2.0|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q| Master|          5|    0|      0.0|           2.0|          3.0|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|33.0|    0|    0|          244373|   13.0|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|    Mrs|          1|    0|      1.0|           0.0|          2.0|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|36.0|    0|    0|            2649|  7.225|       C|    Mrs|          0|    1|      1.0|           1.0|          2.0|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = false)\n",
      " |-- Initial: string (nullable = true)\n",
      " |-- Family_Size: integer (nullable = true)\n",
      " |-- Alone: integer (nullable = false)\n",
      " |-- Sex_index: double (nullable = false)\n",
      " |-- Embarked_index: double (nullable = false)\n",
      " |-- Initial_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"Sex\",\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용할 feature들을 VectorAssembler를 이용해서 벡터화 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = VectorAssembler(inputCols=titanic_df.columns[1:],outputCol=\"features\")\n",
    "feature_vector= feature.transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|            features|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|          2.0|[1.0,38.0,1.0,0.0...|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|          1.0|[3.0,26.0,0.0,0.0...|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|          2.0|[1.0,35.0,1.0,0.0...|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[3....|\n",
      "|       0|     3|33.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|          3.0|[3.0,2.0,3.0,1.0,...|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|          2.0|[3.0,27.0,0.0,2.0...|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|          2.0|[2.0,14.0,1.0,0.0...|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|          1.0|[3.0,4.0,1.0,1.0,...|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|          1.0|[1.0,58.0,0.0,0.0...|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[3....|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|          0.0|[3.0,39.0,1.0,5.0...|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|          1.0|[3.0,14.0,0.0,0.0...|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|          2.0|[2.0,55.0,0.0,0.0...|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|          3.0|[3.0,2.0,4.0,1.0,...|\n",
      "|       1|     2|33.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[2....|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|          2.0|[3.0,31.0,1.0,0.0...|\n",
      "|       1|     3|36.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|[3.0,36.0,0.0,0.0...|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainintData, testData) = feature_vector.randomSplit([0.8, 0.2], seed=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화한 features는 머신러닝 모델에 column으로 한번에 넘길 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       1.0|       0|[1.0,27.0,0.0,2.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       1.0|       0|[1.0,28.0,1.0,0.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       1.0|       0|(10,[0,1,3,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,6],[1.0,...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|[1.0,51.0,0.0,1.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol = 'Survived', featuresCol='features')\n",
    "\n",
    "lrModel = lr.fit(trainintData)\n",
    "lr_prediction = lrModel.transform(testData)\n",
    "lr_prediction.select('prediction','Survived','features').show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol = 'prediction', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegression is = 0.836257\n",
      "Test Error of LogisticRegression = 0.163743 \n"
     ]
    }
   ],
   "source": [
    "lr_accuracy = evaluator.evaluate(lr_prediction)\n",
    "print(\"Accuracy of LogisticRegression is = %g\"% (lr_accuracy))\n",
    "print(\"Test Error of LogisticRegression = %g \" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+-------------+--------------------+----------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|    Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|            features|rawPrediction|         probability|prediction|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+-------------+--------------------+----------+\n",
      "|       0|     1|19.0|    3|    2|   263.0|          5|    0|      0.0|           0.0|          0.0|[1.0,19.0,3.0,2.0...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|27.0|    0|    2|   211.5|          2|    0|      0.0|           1.0|          0.0|[1.0,27.0,0.0,2.0...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|28.0|    0|    0|    47.1|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|28.0|    1|    0| 82.1708|          1|    0|      0.0|           1.0|          0.0|[1.0,28.0,1.0,0.0...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|33.0|    0|    0|    26.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|33.0|    0|    0| 27.7208|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|38.0|    0|    1|153.4625|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,3,4,5],[...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|40.0|    0|    0|     0.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,6],[1.0,...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|40.0|    0|    0| 27.7208|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|45.0|    1|    0|  83.475|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|51.0|    0|    1| 61.3792|          1|    0|      0.0|           1.0|          0.0|[1.0,51.0,0.0,1.0...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|58.0|    0|    0|    29.7|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     1|71.0|    0|    0| 49.5042|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|  [64.0,38.0]|[0.62745098039215...|       0.0|\n",
      "|       0|     2|16.0|    0|    0|    26.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[2....| [299.0,36.0]|[0.89253731343283...|       0.0|\n",
      "|       0|     2|18.0|    0|    0|    73.5|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[2....|    [1.0,4.0]|           [0.2,0.8]|       1.0|\n",
      "|       0|     2|19.0|    0|    0|    13.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[2....| [299.0,36.0]|[0.89253731343283...|       0.0|\n",
      "|       0|     2|21.0|    1|    0|    11.5|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...| [299.0,36.0]|[0.89253731343283...|       0.0|\n",
      "|       0|     2|24.0|    2|    0|    73.5|          2|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|    [4.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|       0|     2|25.0|    1|    0|    26.0|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...| [299.0,36.0]|[0.89253731343283...|       0.0|\n",
      "|       0|     2|26.0|    0|    0|    10.5|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[2....| [299.0,36.0]|[0.89253731343283...|       0.0|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       0.0|       0|[1.0,27.0,0.0,2.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|[1.0,28.0,1.0,0.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,3,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,6],[1.0,...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|[1.0,51.0,0.0,1.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       1.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol='Survived',featuresCol='features')\n",
    "dt_model = dt.fit(trainintData)\n",
    "dt_prediction = dt_model.transform(testData)\n",
    "dt_prediction.select('prediction','Survived','features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier is = 0.807018\n",
      "Test Error of DecisionTreeClassifier = 0.192982 \n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = evaluator.evaluate(dt_prediction)\n",
    "print(\"Accuracy of DecisionTreeClassifier is = %g\"% (dt_accuracy))\n",
    "print(\"Test Error of DecisionTreeClassifier = %g \" % (1.0 - dt_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       0.0|       0|[1.0,27.0,0.0,2.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|[1.0,28.0,1.0,0.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,3,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,6],[1.0,...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|[1.0,51.0,0.0,1.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       1.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(trainintData)\n",
    "rf_prediction = rf_model.transform(testData)\n",
    "rf_prediction.select(\"prediction\", \"Survived\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForestClassifier is = 0.807018\n",
      "Test Error of RandomForestClassifier  = 0.192982 \n"
     ]
    }
   ],
   "source": [
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "print(\"Accuracy of RandomForestClassifier is = %g\"% (rf_accuracy))\n",
    "print(\"Test Error of RandomForestClassifier  = %g \" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       1.0|       0|[1.0,27.0,0.0,2.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       1.0|       0|[1.0,28.0,1.0,0.0...|\n",
      "|       1.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,3,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,6],[1.0,...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|[1.0,51.0,0.0,1.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[2....|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"Survived\", featuresCol=\"features\",maxIter=10)\n",
    "gbt_model = gbt.fit(trainintData)\n",
    "gbt_prediction = gbt_model.transform(testData)\n",
    "gbt_prediction.select(\"prediction\", \"Survived\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gradient-boosted tree classifie is = 0.824561\n",
      "Test Error of Gradient-boosted tree classifie 0.175439\n"
     ]
    }
   ],
   "source": [
    "gbt_accuracy = evaluator.evaluate(gbt_prediction)\n",
    "print(\"Accuracy of Gradient-boosted tree classifie is = %g\"% (gbt_accuracy))\n",
    "print(\"Test Error of Gradient-boosted tree classifie %g\"% (1.0 - gbt_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
